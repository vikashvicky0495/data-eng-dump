{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDD\n",
    "# There are 3 ways for creating RDD in spark \n",
    "#1.parallelize \n",
    "#2.external files \n",
    "#3.from the rdd \n",
    "\n",
    "# Example of the parallelize method\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list that contains all of the elements in this RDD\n",
    "# Note : This method should only be used if the resulting array is expected to be small, \n",
    "# as all the data is loaded into the driver’s memory.\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the number of elements in the RDD\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of partitions\n",
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return an RDD created by coalescing all elements within each partition into a list.\n",
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the above created RDD as the text file\n",
    "rdd1.saveAsTextFile('upgrad_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDD by reading from the file\n",
    "# The file we are trying to read should be present in the hdfs path\n",
    "rddnew = sc.textFile(\"upgrad.txt\")\n",
    "rddnew.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of records in the file\n",
    "rddnew.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a new RDD by applying a function to each element of this RDD.\n",
    "\n",
    "# this is using the lambda functions (anonymous functions)\n",
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "rdd_upper = rdd.map(lambda x: x.upper())\n",
    "rdd_upper.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ... using a regular function\n",
    "def upper_case( v ):\n",
    "    return v.upper()\n",
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "rdd.map(upper_case).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a new RDD containing only the elements that satisfy a predicate\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = rdd.filter(lambda x: x % 2 == 0)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinct Return a new RDD containing the distinct elements in this RDD.\n",
    "sc.parallelize([1, 1, 2, 3]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result returned above will not be sorted, but in the random order\n",
    "# If we want to get the data in sorted order in the driver memory\n",
    "sorted(sc.parallelize([1, 4, 2, 3, 2]).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Union Return the union of this RDD and another one.\n",
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "rdd_union = rdd.union(rdd)\n",
    "rdd_union.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intersection \n",
    "# Return the intersection of this RDD and another one. \n",
    "# The output will not contain any duplicate elements, even if the input RDDs did.\n",
    "\n",
    "# NOTE : This method performs a shuffle internally\n",
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "sorted(rdd1.intersection(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract\n",
    "#Return each value in self that is not contained in other\n",
    "x = sc.parallelize([ 1,2,3,4,5])\n",
    "y = sc.parallelize([2,3,4])\n",
    "sorted(x.subtract(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cartesian\n",
    "#Return the Cartesian product of this RDD and another one, that is, \n",
    "# the RDD of all pairs of elements (a, b) \n",
    "#where a is in self and b is in other.\n",
    "rdd = sc.parallelize([1, 2])\n",
    "rdd2 = sc.parallelize([3,4])\n",
    "sorted(rdd.cartesian(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Action functions ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect\n",
    "#Return a list that contains all of the elements in this RDD.\n",
    "\n",
    "# NOTE : This method should only be used if the resulting array is expected to be small, \n",
    "#as all the data is loaded into the driver’s memory.\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count\n",
    "#Return the number of elements in this RDD.\n",
    "sc.parallelize([2, 3, 4]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countByValue\n",
    "#Return the count of each unique value in this RDD\n",
    "#as a dictionary of (value, count) pairs.\n",
    "sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take(num)\n",
    "# Take the first num elements of the RDD.\n",
    "\n",
    "# It works by first scanning one partition, and use the results from\n",
    "# that partition to estimate the number of additional partitions needed \n",
    "# to satisfy the limit.\n",
    "\n",
    "# Translated from the Scala implementation in RDD#take().\n",
    "\n",
    "\n",
    "#Note this method should only be used if the resulting array is expected to be small, \n",
    "#as all the data is loaded into the driver’s memory.\n",
    "\n",
    "sc.parallelize([2, 3, 4, 5, 6]).take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top(num)\n",
    "#Get the top N elements from an RDD.\n",
    "\n",
    "#Note This method should only be used if the resulting array is \n",
    "#expected to be small, as all the data is loaded into the driver’s memory.\n",
    "\n",
    "#Note It returns the list sorted in descending order.\n",
    "\n",
    "sc.parallelize([2, 3, 4, 5, 6], 2).top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce(function)\n",
    "#Reduces the elements of this RDD using the specified \n",
    "#commutative and associative binary operator. \n",
    "#Currently reduces partitions locally.\n",
    "from operator import add\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
    "\n",
    "sc.parallelize([9,3,1]).reduce(lambda x,y :x/y)\n",
    "# We can pass the custom function inside this reduce function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold\n",
    "#Aggregate the elements of each partition, and then the results \n",
    "# for all the partitions, using a given associative function \n",
    "# and a neutral “zero value.”\n",
    "\n",
    "# The function op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.\n",
    "# from operator import add\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5]).fold(1, lambda x,y :x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate\n",
    "# Aggregate the elements of each partition, and then the results for \n",
    "# all the partitions, using a given combine functions and a neutral\n",
    "# “zero value.”\n",
    "\n",
    "# The functions op(t1, t2) is allowed to modify t1 and return it as \n",
    "# its result value to avoid object allocation; however, it should not modify t2.\n",
    "\n",
    "# The first function (seqOp) can return a different result type, U, \n",
    "# than the type of this RDD. Thus, we need one operation for merging \n",
    "# a T into an U and one operation for merging two U\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "# above step should get us : (1,1),(2,1),(3,1)\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "rdd.aggregate((0,0), seqOp, combOp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foreach\n",
    "def f(x): print(x)\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.foreach(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####   operations on paired rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating paired RDDs\n",
    "# In order to work with paired RDDs it's required to return the RDD which is \n",
    "# composed of tuple\n",
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "pairedrdd= rdd.map(lambda x: (x,1))\n",
    "pairedrdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Transformation functions on one paired RDD’s  #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduceByKey()\n",
    "# Merge the values for each key using an associative and commutative \n",
    "# reduce function.\n",
    "\n",
    "# This will also perform the merging locally on each mapper before sending \n",
    "# results to a reducer, similarly to a “combiner” in MapReduce.\n",
    "\n",
    "# Output will be partitioned with numPartitions partitions, or the default \n",
    "# parallelism level if numPartitions is not specified. Default partitioner \n",
    "# is hash-partition.\n",
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupByKey()\n",
    "# Group the values for each key in the RDD into a single sequence. \n",
    "# Hash-partitions the resulting RDD with numPartitions partitions.\n",
    "\n",
    "# Note If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will provide much better performance.\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.groupByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapValues()\n",
    "#Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD’s partitioning.\n",
    "\n",
    "x = sc.parallelize([(\"a\", [\"Vishwa\", \"Mohan\", \"Rishavv\"]), (\"b\", [\"Abhinav\"])])\n",
    "def f(x): return len(x)\n",
    "x.mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatMapValues()\n",
    "#Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD’s partitioning.\n",
    "x = sc.parallelize([(\"a\", [\"Vishwa\", \"Mohan\", \"Rishavv\"]), (\"b\", [\"Abhinav\", \"Amit\"])])\n",
    "def f(x): return x\n",
    "x.flatMapValues(f).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys()\n",
    "#Return an RDD with the keys of each tuple.\n",
    "m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
    "m.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values()\n",
    "# Return an RDD with the values of each tuple.\n",
    "m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
    "m.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sortByKeys()\n",
    "# Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
    "tmp = [('Apple', 11), ('Banana', 12), ('Mango', 13), ('Carrot', 14), ('Orange', 15)]\n",
    "sc.parallelize(tmp).sortByKey(True, 1).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########    Transformation functions on two paired RDD    ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtractByKey()\n",
    "#Return each (key, value) pair in self that has no pair with matching key in other.\n",
    "\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtractByKey(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join()\n",
    "\n",
    "# Return an RDD containing all pairs of elements with matching keys in self\n",
    "# and other.\n",
    "\n",
    "# Each pair of elements will be returned as a (k, (v1, v2)) tuple,\n",
    "# where (k, v1) is in self and (k, v2) is in other.\n",
    "\n",
    "# Performs a hash join across the cluster.\n",
    "\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rightOuterJoin()\n",
    "\n",
    "# Perform a right outer join of self and other.\n",
    "\n",
    "# For each element (k, w) in other, the resulting RDD will either contain\n",
    "# all pairs (k, (v, w)) for v in this, or the pair (k, (None, w)) \n",
    "# if no elements in self have key k.\n",
    "\n",
    "# Hash-partitions the resulting RDD into the given number of partitions.\n",
    "\n",
    "rdd1 = sc.parallelize([(\"a\", True), (\"b\", True)])\n",
    "rdd2 = sc.parallelize([(\"a\", False)])\n",
    "sorted(rdd2.rightOuterJoin(rdd1).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leftOuterJoin()\n",
    "# Perform a left outer join of self and other.\n",
    "\n",
    "# For each element (k, v) in self, the resulting RDD will either contain \n",
    "# all pairs (k, (v, w)) for w in other, or the pair (k, (v, None)) if no\n",
    "# elements in other have key k.\n",
    "\n",
    "# Hash-partitions the resulting RDD into the given number of partitions.\n",
    "\n",
    "rdd1 = sc.parallelize([(\"a\", True), (\"b\", True)])\n",
    "rdd2 = sc.parallelize([(\"a\", False)])\n",
    "sorted(rdd2.leftOuterJoin(rdd1).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cogroup()\n",
    "# For each key k in self or other, return a resulting RDD that contains a \n",
    "# tuple with the list of values for that key in self as well as other.\n",
    "\n",
    "rdd1 = sc.parallelize([(\"a\", True), (\"b\", True)])\n",
    "rdd2 = sc.parallelize([(\"a\", False)])\n",
    "\n",
    "# rdd1.cogroup(rdd2).collect()  Below will be the result of the cogroup\n",
    "# [('a',\n",
    "#   (<pyspark.resultiterable.ResultIterable at 0x7f5c5c0dfed0>,\n",
    "#    <pyspark.resultiterable.ResultIterable at 0x7f5c5c1c1450>)),\n",
    "#  ('b',\n",
    "#   (<pyspark.resultiterable.ResultIterable at 0x7f5c5c07bbd0>,\n",
    "#    <pyspark.resultiterable.ResultIterable at 0x7f5c5c07bc50>))]\n",
    "\n",
    "\n",
    "[(rdd1, tuple(map(list, rdd2))) for rdd1, rdd2 in sorted(list(rdd1.cogroup(rdd2).collect()))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############      Action on paired RDD   ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countByKey()\n",
    "# Count the number of elements for each key, and return the result to \n",
    "# the master as a dictionary.\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup(key)\n",
    "# Return the list of values in the RDD for key key. \n",
    "# This operation is done efficiently if the RDD has a known partitioner by\n",
    "# only searching the partition that the key maps to.\n",
    "\n",
    "l = range(1000)\n",
    "rdd = sc.parallelize(zip(l, l), 10)\n",
    "rdd.lookup(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
